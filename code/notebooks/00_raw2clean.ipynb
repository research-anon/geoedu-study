{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4925431a",
   "metadata": {},
   "source": [
    "**Objetivo:** Leer y estandarizar los datasets (SIES, DEMRE, IVM, georreferenciación, etc.)\n",
    "\n",
    "- Cargar datos de todas las fuentes\n",
    "- Verificar formatos y columnas clave\n",
    "- Establecer claves de unión (`MRUN`, `RBD`, códigos de institución)\n",
    "- Guardar versiones limpias en `data/processed/` (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ca167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Configuraciones generales\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83734ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de regiones unificado\n",
    "region_dict = {\n",
    "    'REGION DE ARICA Y PARINACOTA': 'Arica y Parinacota',\n",
    "    'REGION DE TARAPACA': 'Tarapacá',\n",
    "    'REGION DE ANTOFAGASTA': 'Antofagasta',\n",
    "    'REGION DE ATACAMA': 'Atacama',\n",
    "    'REGION DE COQUIMBO': 'Coquimbo',\n",
    "    'REGION DE VALPARAISO': 'Valparaíso',\n",
    "    \"REGION DEL LIBERTADOR GENERAL BERNARDO O'HIGGINS\": \"Lib. Gral B. O'Higgins\",\n",
    "    'REGION DEL MAULE': 'Maule',\n",
    "    'REGION DE ÑUBLE': 'Ñuble',\n",
    "    'REGION DEL BIOBIO': 'Biobío',\n",
    "    'REGION DE LA ARAUCANIA': 'La Araucanía',\n",
    "    'REGION DE LOS RIOS': 'Los Ríos',\n",
    "    'REGION DE LOS LAGOS': 'Los Lagos',\n",
    "    'REGION AISEN DEL GENERAL CARLOS IBAÑEZ DEL CAMPO': 'Aysén',\n",
    "    'REGION DE MAGALLANES Y DE LA ANTARTICA CHILENA': 'Magallanes',\n",
    "    'REGION METROPOLITANA DE SANTIAGO': 'Metropolitana',\n",
    "    'REGIÓN DE ARICA Y PARINACOTA': 'Arica y Parinacota',\n",
    "    'REGIÓN DE TARAPACÁ': 'Tarapacá',\n",
    "    'REGIÓN DE ANTOFAGASTA': 'Antofagasta',\n",
    "    'REGIÓN DE ATACAMA': 'Atacama',\n",
    "    'REGIÓN DE COQUIMBO': 'Coquimbo',\n",
    "    'REGIÓN DE VALPARAÍSO': 'Valparaíso',\n",
    "    'REGIÓN DEL LIBERTADOR GRAL. BERNARDO O\\'HIGGINS': \"Lib. Gral B. O'Higgins\",\n",
    "    'REGIÓN DEL MAULE': 'Maule',\n",
    "    'REGIÓN DE ÑUBLE': 'Ñuble',\n",
    "    'REGIÓN DEL BIOBÍO': 'Biobío',\n",
    "    'REGIÓN DE LA ARAUCANÍA': 'La Araucanía',\n",
    "    'REGIÓN DE LOS RÍOS': 'Los Ríos',\n",
    "    'REGIÓN DE LOS LAGOS': 'Los Lagos',\n",
    "    'REGIÓN DE AYSÉN DEL GRAL. CARLOS IBÁÑEZ DEL CAMPO': 'Aysén',\n",
    "    'REGIÓN DE MAGALLANES Y DE LA ANTÁRTICA CHILENA': 'Magallanes',\n",
    "    'REGIÓN METROPOLITANA DE SANTIAGO': 'Metropolitana'\n",
    "}\n",
    "\n",
    "# Orden de regiones para visualización\n",
    "orden_regiones = [\n",
    "    'Arica y Parinacota', 'Tarapacá', 'Antofagasta', 'Atacama', 'Coquimbo',\n",
    "    'Valparaíso', 'Metropolitana', \"Lib. Gral B. O'Higgins\", 'Maule', 'Ñuble',\n",
    "    'Biobío', 'La Araucanía', 'Los Ríos', 'Los Lagos', 'Aysén', 'Magallanes'\n",
    "]\n",
    "\n",
    "# Diccionario para nombres de TIPO_DEPEN\n",
    "tipodepen_dict = {\n",
    "    1: 'Municipal',\n",
    "    2: 'Particular Subvencionado',\n",
    "    3: 'Particular Pagado',\n",
    "    4: 'Corp. Administración Delegada',\n",
    "    5: 'Servicio Local de Educación'\n",
    "}\n",
    "\n",
    "# Mapear códigos de región para set_C\n",
    "regiones_dict_inv = {\n",
    "    15: 'Arica y Parinacota',\n",
    "    1: 'Tarapacá',\n",
    "    2: 'Antofagasta',\n",
    "    3: 'Atacama',\n",
    "    4: 'Coquimbo',\n",
    "    5: 'Valparaíso',\n",
    "    6: 'Metropolitana',\n",
    "    7: \"Lib. Gral B. O'Higgins\",\n",
    "    8: 'Maule',\n",
    "    16: 'Ñuble',\n",
    "    9: 'Biobío',\n",
    "    10: 'La Araucanía',\n",
    "    11: 'Los Ríos',\n",
    "    12: 'Los Lagos',\n",
    "    13: 'Aysén',\n",
    "    14: 'Magallanes'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07054f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_paths_anio(path_base: Path, anio: int) -> dict:\n",
    "    path_anio = path_base / str(anio)\n",
    "\n",
    "    path_matricula = path_anio / f'20230802_Matrícula_Ed_Superior_{anio}_PUBL_MRUN.csv'\n",
    "    path_puntajes = (\n",
    "        path_anio / f'A_INSCRITOS_PUNTAJES_PDT_{anio}_PUB_MRUN.csv'\n",
    "        if anio in [2021, 2022]\n",
    "        else path_anio / f'A_INSCRITOS_PUNTAJES_{anio}_PAES_PUB_MRUN.csv'\n",
    "    )\n",
    "    path_ivm = path_anio / f'IVM_Establecimientos_{anio}.xlsx'\n",
    "\n",
    "    # Paths fijos para shapefiles\n",
    "    path_establecimientos = path_anio / 'establecimientos' / 'layer_establecimientos_educacion_escolar_20220309024120.shp'\n",
    "    path_inmuebles = path_anio / 'inmuebles_ies' / 'layer_establecimientos_de_educacion_superior_20220309024111.shp'\n",
    "\n",
    "    return {\n",
    "        \"matricula\": path_matricula,\n",
    "        \"puntajes\": path_puntajes,\n",
    "        \"ivm\": path_ivm,\n",
    "        \"establecimientos\": path_establecimientos,\n",
    "        \"inmuebles_ies\": path_inmuebles\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccb647c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_conjunto_a_desde_path(path_set_A: Path, region_dict: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Lee y transforma el set A desde un path específico.\n",
    "\n",
    "    Parámetros:\n",
    "    ----------\n",
    "    path_set_A : Path\n",
    "        Ruta al archivo CSV.\n",
    "    region_dict : dict\n",
    "        Diccionario para estandarizar nombres de región.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "    tuple\n",
    "        DataFrame procesado y año extraído desde el nombre del archivo.\n",
    "    \"\"\"\n",
    "\n",
    "    columnas_clave = [\n",
    "        \"mrun\", \"region_sede\", \"provincia_sede\", \"comuna_sede\",\"cod_inst\",\n",
    "        \"nomb_inst\", \"nomb_carrera\", \"tipo_inst_1\",\"tipo_inst_2\",\"tipo_inst_3\",\n",
    "        \"nivel_global\", \"nivel_carrera_1\", \"anio_ing_carr_act\", \"anio_ing_carr_ori\",\n",
    "        \"forma_ingreso\", \"rango_edad\"\n",
    "    ]\n",
    "\n",
    "    df = pd.read_csv(path_set_A, sep=';', encoding='utf-8', usecols=columnas_clave)\n",
    "    df['NOMBRE_REGION_INGRESO'] = df['region_sede'].replace(region_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generar_conjuntos_filtrados(df: pd.DataFrame, year_A: int) -> dict:\n",
    "    \"\"\"\n",
    "    Genera subconjuntos filtrados desde un DataFrame base.\n",
    "\n",
    "    Parámetros:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame base.\n",
    "    year_A : int\n",
    "        Año de ingreso a utilizar en los filtros.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "    dict\n",
    "        Conjuntos filtrados por nombre.\n",
    "    \"\"\"\n",
    "    filtros_a = {\n",
    "        \"A0\": {\n",
    "            \"anio_ing_carr_ori\": year_A,\n",
    "            \"nivel_global\": \"Pregrado\",\n",
    "            \"forma_ingreso\": \"1- Ingreso Directo (regular)\"\n",
    "        },\n",
    "        \"A\": {\n",
    "            \"anio_ing_carr_ori\": year_A,\n",
    "            \"nivel_global\": \"Pregrado\",\n",
    "            \"forma_ingreso\": \"1- Ingreso Directo (regular)\",\n",
    "            \"rango_edad\": \"15 a 19 años\"\n",
    "        },\n",
    "        \"A1\": {\n",
    "            \"anio_ing_carr_ori\": year_A,\n",
    "            \"nivel_global\": \"Pregrado\",\n",
    "            \"forma_ingreso\": \"1- Ingreso Directo (regular)\",\n",
    "            \"rango_edad\": \"15 a 19 años\",\n",
    "            \"tipo_inst_1\": \"Universidades\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def aplicar_filtro(df, condiciones):\n",
    "        for columna, valor in condiciones.items():\n",
    "            df = df[df[columna] == valor]\n",
    "        return df\n",
    "\n",
    "    resultados_filtrados = {\n",
    "        nombre: aplicar_filtro(df, condiciones)\n",
    "        for nombre, condiciones in filtros_a.items()\n",
    "    }\n",
    "\n",
    "    return resultados_filtrados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76f49691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def leer_conjunto_b_desde_path(path_set_B: Path, region_dict: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Lee y transforma el set B desde un path específico.\n",
    "\n",
    "    Parámetros:\n",
    "    ----------\n",
    "    path_set_B : Path\n",
    "        Ruta al archivo CSV.\n",
    "    region_dict : dict\n",
    "        Diccionario para estandarizar nombres de región.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "    tuple\n",
    "        DataFrame procesado y año extraído desde el nombre del archivo.\n",
    "    \"\"\"\n",
    "    year_B = int(re.search(r'_(\\d{4})_', str(path_set_B)).group(1))\n",
    "\n",
    "    columnas_B = [\n",
    "        \"MRUN\", \"RBD\", \"CODIGO_REGION_EGRESO\", \"NOMBRE_REGION_EGRESO\",\n",
    "        \"PTJE_RANKING\", \"PTJE_NEM\"\n",
    "    ] + [\"PROM_CM_ACTUAL\" if year_B < 2023 else \"PROMEDIO_CM_MAX\"]\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path_set_B,\n",
    "        sep=';',\n",
    "        encoding='utf-8',\n",
    "        dtype={'RBD': str, 'NOMBRE_REGION_EGRESO': str},\n",
    "        usecols=columnas_B\n",
    "    ).rename(columns={'MRUN': 'mrun', 'PROMEDIO_CM_MAX': 'PROM_CM_ACTUAL'})\n",
    "\n",
    "    df['NOMBRE_REGION_EGRESO'] = df['NOMBRE_REGION_EGRESO'].replace(region_dict)\n",
    "    df['RBD'] = pd.to_numeric(df['RBD'], errors='coerce').dropna().astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1393251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def leer_conjunto_c_desde_path(path_set_C: Path) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Lee y transforma el set C (establecimientos escolares) desde un archivo shapefile.\n",
    "\n",
    "    Parámetros:\n",
    "    ----------\n",
    "    path_set_C : Path\n",
    "        Ruta al archivo `.shp`.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        GeoDataFrame con las columnas procesadas.\n",
    "    \"\"\"\n",
    "    columnas_C = [\"RBD\", \"NOM_RBD\", \"COD_REG_RB\", \"TIPO_DEPEN\", \"LATITUD\", \"LONGITUD\"]\n",
    "\n",
    "    df = gpd.read_file(path_set_C)[columnas_C].rename(\n",
    "        columns={'LATITUD': 'LATITUD_COL', 'LONGITUD': 'LONGITUD_COL'}\n",
    "    )\n",
    "\n",
    "    df['RBD'] = pd.to_numeric(df['RBD'], errors='coerce').dropna().astype(int)\n",
    "    df['COD_REG_RB'] = pd.to_numeric(df['COD_REG_RB'], errors='coerce')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac571106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_conjunto_d_desde_path(path_set_D: Path, region_dict: dict) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Lee y transforma el set D (inmuebles de educación superior) desde un archivo shapefile.\n",
    "\n",
    "    Parámetros:\n",
    "    ----------\n",
    "    path_set_D : Path\n",
    "        Ruta al archivo `.shp`.\n",
    "    region_dict : dict\n",
    "        Diccionario de estandarización de regiones.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        GeoDataFrame procesado.\n",
    "    \"\"\"\n",
    "    columnas_D = [\"NOMBRE_INS\", \"REGIÓN\", \"COMUNA\", \"LATITUD\", \"LONGITUD\", \"TIPO_INST\"]\n",
    "\n",
    "    df = gpd.read_file(path_set_D)[columnas_D].rename(\n",
    "        columns={'LATITUD': 'LATITUD_UNI', 'LONGITUD': 'LONGITUD_UNI'}\n",
    "    )\n",
    "\n",
    "    df['TIPO_INST'] = df['TIPO_INST'].str.capitalize()\n",
    "    df['REGIÓN'] = df['REGIÓN'].replace(region_dict)\n",
    "\n",
    "    df = df.drop_duplicates(subset=['NOMBRE_INS', 'REGIÓN', 'COMUNA'], keep='first')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2bd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def leer_conjunto_e_desde_path(path_set_E: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee y transforma el set E (IVM) desde un archivo Excel.\n",
    "\n",
    "    Parámetros:\n",
    "    ----------\n",
    "    path_set_E : Path\n",
    "        Ruta al archivo `.xlsx`.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame procesado con columnas agrupadas y valor de corte por año.\n",
    "    \"\"\"\n",
    "    # Extraer año del nombre del archivo\n",
    "    year_E = int(re.search(r'Establecimientos_(\\d{4})', str(path_set_E)).group(1))\n",
    "\n",
    "    # Leer hoja \"Media\"\n",
    "    df = pd.read_excel(path_set_E, sheet_name='Media')\n",
    "\n",
    "    # Procesar duplicados\n",
    "    df = (\n",
    "        df.groupby(\"ID_RBD\", group_keys=False)\n",
    "        .agg({\n",
    "            \"N EVALUADO\": \"sum\",\n",
    "            \"IVM Bajo\": \"sum\",\n",
    "            \"IVM Medio\": \"sum\",\n",
    "            \"IVM Alto\": \"sum\",\n",
    "            \"IVM Muy Alto\": \"sum\"\n",
    "        })\n",
    "        .assign(\n",
    "            IVM_Establecimiento=lambda df: (\n",
    "                (df.loc[df['ID_RBD'].isin(df.index), \"IVM Establecimiento\"] * \n",
    "                 df.loc[df['ID_RBD'].isin(df.index), \"N EVALUADO\"]).groupby(df[\"ID_RBD\"]).sum()\n",
    "                / df[\"N EVALUADO\"]\n",
    "            )\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Definir alta vulnerabilidad por año\n",
    "    filtros_e = {\n",
    "        2020: 21.42601 ,\n",
    "        2021: 20.03805,\n",
    "        2022: 20.03805,\n",
    "        2023: 18.33834,\n",
    "        2024: 19.35458\n",
    "    }\n",
    "\n",
    "\n",
    "    df['valor_corte'] = filtros_e[year_E]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b2c1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_university_coordinates(df, universities_db):\n",
    "    \"\"\"\n",
    "    Llena coordenadas de universidades faltantes usando matching jerárquico.\n",
    "    \"\"\"\n",
    "\n",
    "    na_rows = df[df['LATITUD_UNI'].isna()]\n",
    "    if na_rows.empty:\n",
    "        print(\"No hay valores NA en las coordenadas universitarias\")\n",
    "        return df\n",
    "\n",
    "    print(f\"\\nFilling coordinates for {len(na_rows)} missing entries...\")\n",
    "    fill_count = 0\n",
    "\n",
    "    for index, row in na_rows.iterrows():\n",
    "        match = universities_db[\n",
    "            (universities_db['NOMBRE_INS'] == row['nomb_inst']) &\n",
    "            (universities_db['REGIÓN'] == row['NOMBRE_REGION_INGRESO']) &\n",
    "            (universities_db['COMUNA'] == row['comuna_sede'])\n",
    "        ]\n",
    "\n",
    "        if match.empty:\n",
    "            match = universities_db[\n",
    "                (universities_db['NOMBRE_INS'] == row['nomb_inst']) &\n",
    "                (universities_db['REGIÓN'] == row['NOMBRE_REGION_INGRESO'])\n",
    "            ]\n",
    "\n",
    "        if match.empty:\n",
    "            match = universities_db[\n",
    "                (universities_db['NOMBRE_INS'] == row['nomb_inst']) &\n",
    "                (universities_db['COMUNA'] == row['comuna_sede'])\n",
    "            ]\n",
    "\n",
    "        if match.empty:\n",
    "            match = universities_db[\n",
    "                (universities_db['REGIÓN'] == row['NOMBRE_REGION_INGRESO']) &\n",
    "                (universities_db['COMUNA'] == row['comuna_sede'])\n",
    "            ]\n",
    "\n",
    "        if not match.empty:\n",
    "            df.at[index, 'LATITUD_UNI'] = match['LATITUD'].iloc[0]\n",
    "            df.at[index, 'LONGITUD_UNI'] = match['LONGITUD'].iloc[0]\n",
    "            fill_count += 1\n",
    "\n",
    "    print(f\"Successfully filled {fill_count} missing coordinates\")\n",
    "    print(f\"Remaining NA values: {df['LATITUD_UNI'].isna().sum()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calcula la distancia Haversine entre dos puntos.\n",
    "    \"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    return 6371.0 * 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "\n",
    "def generar_conjuntos_abcde(set_a: pd.DataFrame,\n",
    "                             set_b: pd.DataFrame,\n",
    "                             set_c: pd.DataFrame,\n",
    "                             set_d: pd.DataFrame,\n",
    "                             set_e: pd.DataFrame,\n",
    "                             output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Junta y guarda los conjuntos AB, ABC, ABCD y ABCDE.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # AB\n",
    "    set_ab = pd.merge(set_b, set_a, on=\"mrun\", how=\"inner\").loc[lambda x: x['NOMBRE_REGION_EGRESO'] != ' ']\n",
    "\n",
    "    # ABC\n",
    "    set_abc = pd.merge(set_ab, set_c, on='RBD', how='inner')\n",
    "\n",
    "    # ABCD\n",
    "    set_abcd = pd.merge(\n",
    "        set_abc.rename(columns={'LATITUD': 'LATITUD_COL', 'LONGITUD': 'LONGITUD_COL'}),\n",
    "        set_d.rename(columns={'LATITUD': 'LATITUD_UNI', 'LONGITUD': 'LONGITUD_UNI'}),\n",
    "        left_on=['nomb_inst', 'NOMBRE_REGION_INGRESO', 'comuna_sede'],\n",
    "        right_on=['NOMBRE_INS', 'REGIÓN', 'COMUNA'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    set_abcd2 = fill_university_coordinates(\n",
    "        set_abcd,\n",
    "        set_d.rename(columns={'LATITUD_UNI': 'LATITUD', 'LONGITUD_UNI': 'LONGITUD'})\n",
    "    )\n",
    "\n",
    "    # ABCDE\n",
    "    set_abcde = pd.merge(set_abcd2, set_e, left_on='RBD', right_on='ID_RBD', how='left')\n",
    "\n",
    "    valor_corte = set_e['valor_corte'].iloc[0]\n",
    "    set_abcde.loc[\n",
    "        (set_abcde[\"IVM_Establecimiento\"].isnull()) & (set_abcde[\"TIPO_DEPEN\"] == 3),\n",
    "        \"IVM_Establecimiento\"\n",
    "    ] = valor_corte - 1\n",
    "\n",
    "    set_abcde['DISTANCIA'] = set_abcde.apply(\n",
    "        lambda row: haversine(row['LATITUD_COL'], row['LONGITUD_COL'], row['LATITUD_UNI'], row['LONGITUD_UNI']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    print(f\"Cantidad de observaciones: {set_abcde.shape[0]}\")\n",
    "\n",
    "    set_ab.to_csv(f\"{output_path}/set_ab.csv\", index=False)\n",
    "    set_abc.to_csv(f\"{output_path}/set_abc.csv\", index=False)\n",
    "    set_abcd.to_csv(f\"{output_path}/set_abcd.csv\", index=False)\n",
    "    set_abcde.to_csv(f\"{output_path}/set_abcde.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7709ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Procesando año 2021...\n",
      "\n",
      "Filling coordinates for 50976 missing entries...\n",
      "Successfully filled 33634 missing coordinates\n",
      "Remaining NA values: 17342\n",
      "Cantidad de observaciones: 77366\n",
      "✅ Año 2021 completado. Datos guardados en ..\\data\\clean\\2021\n",
      "\n",
      "📁 Procesando año 2022...\n",
      "\n",
      "Filling coordinates for 52760 missing entries...\n",
      "Successfully filled 35130 missing coordinates\n",
      "Remaining NA values: 17630\n",
      "Cantidad de observaciones: 79386\n",
      "✅ Año 2022 completado. Datos guardados en ..\\data\\clean\\2022\n",
      "\n",
      "📁 Procesando año 2023...\n",
      "\n",
      "Filling coordinates for 61271 missing entries...\n",
      "Successfully filled 41908 missing coordinates\n",
      "Remaining NA values: 19363\n",
      "Cantidad de observaciones: 92470\n",
      "✅ Año 2023 completado. Datos guardados en ..\\data\\clean\\2023\n",
      "\n",
      "📁 Procesando año 2024...\n",
      "\n",
      "Filling coordinates for 61062 missing entries...\n",
      "Successfully filled 41280 missing coordinates\n",
      "Remaining NA values: 19782\n",
      "Cantidad de observaciones: 92699\n",
      "✅ Año 2024 completado. Datos guardados en ..\\data\\clean\\2024\n"
     ]
    }
   ],
   "source": [
    "# Ruta base\n",
    "path_base = Path('../data/raw')\n",
    "years = [2021, 2022, 2023, 2024]\n",
    "\n",
    "for year in years:\n",
    "    print(f\"\\n📁 Procesando año {year}...\")\n",
    "\n",
    "    # Obtener paths del año\n",
    "    path_year = obtener_paths_anio(path_base, year)\n",
    "\n",
    "    # Leer conjuntos individuales\n",
    "    set_a = leer_conjunto_a_desde_path(path_year['matricula'], region_dict)\n",
    "    set_b = leer_conjunto_b_desde_path(path_year['puntajes'], region_dict)\n",
    "    set_c = leer_conjunto_c_desde_path(path_year['establecimientos'])\n",
    "    set_d = leer_conjunto_d_desde_path(path_year['inmuebles_ies'], region_dict)\n",
    "    set_e = leer_conjunto_e_desde_path(path_year['ivm'])\n",
    "\n",
    "    # Filtrar A1\n",
    "    resultados_a = generar_conjuntos_filtrados(set_a, year)\n",
    "    set_a0 = resultados_a['A0']\n",
    "    set_aa = resultados_a['A']\n",
    "    set_a1 = resultados_a['A1']\n",
    "\n",
    "\n",
    "    # Carpeta de salida\n",
    "    output_path = Path(f'../data/clean/{year}')\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Guardar conjuntos individuales\n",
    "    set_a.to_csv(output_path / 'set_a.csv', index=False)\n",
    "    set_a0.to_csv(output_path / 'set_a0.csv', index=False)\n",
    "    set_aa.to_csv(output_path / 'set_aa.csv', index=False)\n",
    "    set_a1.to_csv(output_path / 'set_a1.csv', index=False)\n",
    "    set_b.to_csv(output_path / 'set_b.csv', index=False)\n",
    "    set_c.to_csv(output_path / 'set_c.csv', index=False)\n",
    "    set_d.to_csv(output_path / 'set_d.csv', index=False)\n",
    "    set_e.to_csv(output_path / 'set_e.csv', index=False)\n",
    "\n",
    "    # Generar y guardar conjuntos integrados\n",
    "    generar_conjuntos_abcde(\n",
    "        set_a=set_a1,\n",
    "        set_b=set_b,\n",
    "        set_c=set_c,\n",
    "        set_d=set_d,\n",
    "        set_e=set_e,\n",
    "        output_path=output_path\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Año {year} completado. Datos guardados en {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
